{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch dataloader for loading the preprocessed data and turning it into torch tensors that can be fed into the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/extracted_comb.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title has maximum 39 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "for num, i in enumerate(data[:,0]):\n",
    "    if maximum < len(i):\n",
    "        maximum = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title has maximum 129 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "for num, i in enumerate(data[:,1]):\n",
    "    if maximum < len(i):\n",
    "        maximum = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For some reason text isn't uniformly 50 words long!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for num, i in enumerate(data[:,1]):\n",
    "    if num == 2:\n",
    "        break\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be done:\n",
    "\n",
    "* add starting and ending words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = 0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, max_title=39, max_text=129):\n",
    "        '''\n",
    "        Args:\n",
    "        root_dir (string): Path to npy directory\n",
    "        ids (string): Path to csv file containing list of patient ids for training/testing\n",
    "        diag_vocab (string): Path to csv file containing all diagnose labels\n",
    "        max_visits (int): Maximum number of visits\n",
    "        max_diag (int): Maximum number of diagnoses per visit that are considered\n",
    "        Outputs:\n",
    "        inputs (torch.tensor): Concatenated info of diagnoses and times between visits (batch_size, max_visits, vocab_size+1)\n",
    "        labels (torch.tensor): Labels for visits\n",
    "        '''\n",
    "        self.data = np.load(root_dir, allow_pickle=True) \n",
    "        self.max_len_title = max_title\n",
    "        self.max_len_text = max_text\n",
    "        self.word2idx, self.idx2word = self.indexify_vocab(self.data)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #load text and title data(these are lists)\n",
    "        title = self.data[idx, 0]\n",
    "        text = self.data[idx, 1]\n",
    "        \n",
    "        #indexify title and text\n",
    "        title = self.indexify_title(title, self.max_len_title)\n",
    "        text = self.indexify_text(text, self.max_len_text)\n",
    "        \n",
    "        #convert to tensors\n",
    "        title = torch.from_numpy(title).to(device)\n",
    "        text = torch.from_numpy(text).to(device)\n",
    "        \n",
    "        return title, text\n",
    "    \n",
    "    def indexify_vocab(self, vocab):\n",
    "        '''\n",
    "        function for creating word2idx and idx2word dictionaries of the vocabulary\n",
    "        Arg:\n",
    "        vocab (numpy array): data containing titles(on column 0) and text(on column 1)\n",
    "        Return: \n",
    "        word2idx (dictionary): Words linked with unique index\n",
    "        idx2word (dictionary): Indeces linked with unique words\n",
    "        '''\n",
    "        word2idx = dict()\n",
    "        idx2word = dict()\n",
    "        word2idx['<s>'] = 1\n",
    "        idx2word[1] = '<s>'\n",
    "        word2idx['</s>'] = 2\n",
    "        idx2word[2] = '</s>'\n",
    "        index = 3\n",
    "        for num, i in enumerate(data[:,0]): #loop for titles\n",
    "            for j in i:\n",
    "                if j not in word2idx.keys():\n",
    "                    word2idx[j] = index\n",
    "                    idx2word[index] = j\n",
    "                    index += 1\n",
    "        \n",
    "        for num, i in enumerate(data[:,1]): #loop for text\n",
    "            for j in i:\n",
    "                if j not in word2idx.keys():\n",
    "                    word2idx[j] = index\n",
    "                    idx2word[index] = j\n",
    "                    index += 1\n",
    "        \n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    def indexify_text(self, text, max_len):\n",
    "        '''\n",
    "        Arg:\n",
    "        text (list): Contains text in list form\n",
    "        Return:\n",
    "        ret (numpy.array): Indexes of words in text\n",
    "        '''\n",
    "        ret = np.full((max_len), padding_value) \n",
    "        for i in range(len(text)):\n",
    "            ret[i] = self.word2idx[text[i]]\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    def indexify_title(self, text, max_len):\n",
    "        '''\n",
    "        Arg:\n",
    "        text (list): Contains title in list form\n",
    "        Return:\n",
    "        ret (numpy.array): Indexes of words in text\n",
    "        '''\n",
    "        ret = np.full((max_len+1), padding_value) \n",
    "        for i in range(len(text)):\n",
    "            ret[i+1] = self.word2idx[text[i]]\n",
    "            \n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3494722843170166\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ds = CustomDataset(\"data/extracted_comb.npy\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = torch.utils.data.random_split(ds, [round(len(data)*0.2), round(len(data)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x21e384c6fc8>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  3075,  3076,  ...,     0,     0,     0],\n",
      "        [    0,  5641, 22256,  ...,     0,     0,     0],\n",
      "        [    0,   611,   972,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,    14,    15,  ...,     0,     0,     0],\n",
      "        [    0,  1781,  1782,  ...,     0,     0,     0],\n",
      "        [    0,    13,   292,  ...,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "torch.Size([32, 40])\n",
      "tensor([[  629,  2252,  1205,  ...,     0,     0,     0],\n",
      "        [  138,  1705,   140,  ...,     0,     0,     0],\n",
      "        [  809,   388,   134,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   13,  5793,    75,  ...,     0,     0,     0],\n",
      "        [ 5963, 15094,    69,  ...,     0,     0,     0],\n",
      "        [ 2800,  3860,   445,  ...,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "torch.Size([32, 129])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    if i==1:\n",
    "        break\n",
    "    print(data[0])\n",
    "    print(data[0].shape)\n",
    "    print(data[1])\n",
    "    print(data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
