{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch dataloader for loading the preprocessed data and turning it into torch tensors that can be fed into the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/extracted_comb.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title has maximum 39 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "for num, i in enumerate(data[:,0]):\n",
    "    if maximum < len(i):\n",
    "        maximum = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text has maximum 129 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "for num, i in enumerate(data[:,1]):\n",
    "    if maximum < len(i):\n",
    "        maximum = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For some reason text isn't uniformly 50 words long!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for num, i in enumerate(data[:,1]):\n",
    "    if num == 2:\n",
    "        break\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be done:\n",
    "\n",
    "* add starting and ending words (DONE)\n",
    "\n",
    "* mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = 0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, max_title=39, max_text=129):\n",
    "        '''\n",
    "        Args:\n",
    "        root_dir (string): Path to npy directory\n",
    "        ids (string): Path to csv file containing list of patient ids for training/testing\n",
    "        diag_vocab (string): Path to csv file containing all diagnose labels\n",
    "        max_visits (int): Maximum number of visits\n",
    "        max_diag (int): Maximum number of diagnoses per visit that are considered\n",
    "        Outputs:\n",
    "        inputs (torch.tensor): Concatenated info of diagnoses and times between visits (batch_size, max_visits, vocab_size+1)\n",
    "        labels (torch.tensor): Labels for visits\n",
    "        '''\n",
    "        self.data = np.load(root_dir, allow_pickle=True) \n",
    "        self.max_len_title = max_title\n",
    "        self.max_len_text = max_text\n",
    "        self.word2idx, self.idx2word = self.indexify_vocab(self.data)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #load text and title data(these are lists) and add start and stop tokens\n",
    "        title = self.add_start_stop(self.data[idx, 0])\n",
    "        text = self.add_start_stop(self.data[idx, 1])\n",
    "        \n",
    "        # masking for text (+2 in torch.ones comes from adding start and stop tokens)\n",
    "        mask = torch.cat((torch.zeros(len(text)), torch.ones(2 + self.max_len_text - len(text)))).bool().to(device)\n",
    "        \n",
    "        #indexify title and text\n",
    "        title = self.indexify_title(title, self.max_len_title)\n",
    "        text = self.indexify_text(text, self.max_len_text)\n",
    "        \n",
    "        #convert to tensors\n",
    "        title = torch.from_numpy(title).to(device)\n",
    "        text = torch.from_numpy(text).to(device)\n",
    "        \n",
    "        return title, text, mask\n",
    "    \n",
    "    def add_start_stop(self, text):\n",
    "        ret = '<s> ' + ' '.join(text) + ' </s>'\n",
    "        return ret.split()\n",
    "    \n",
    "    def indexify_vocab(self, vocab):\n",
    "        '''\n",
    "        function for creating word2idx and idx2word dictionaries of the vocabulary\n",
    "        Arg:\n",
    "        vocab (numpy array): data containing titles(on column 0) and text(on column 1)\n",
    "        Return: \n",
    "        word2idx (dictionary): Words linked with unique index\n",
    "        idx2word (dictionary): Indeces linked with unique words\n",
    "        '''\n",
    "        word2idx = dict()\n",
    "        idx2word = dict()\n",
    "        word2idx['<s>'] = 1\n",
    "        idx2word[1] = '<s>'\n",
    "        word2idx['</s>'] = 2\n",
    "        idx2word[2] = '</s>'\n",
    "        index = 3\n",
    "        for num, i in enumerate(data[:,0]): #loop for titles\n",
    "            for j in i:\n",
    "                if j not in word2idx.keys():\n",
    "                    word2idx[j] = index\n",
    "                    idx2word[index] = j\n",
    "                    index += 1\n",
    "        \n",
    "        for num, i in enumerate(data[:,1]): #loop for text\n",
    "            for j in i:\n",
    "                if j not in word2idx.keys():\n",
    "                    word2idx[j] = index\n",
    "                    idx2word[index] = j\n",
    "                    index += 1\n",
    "        \n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    def indexify_text(self, text, max_len):\n",
    "        '''\n",
    "        Arg:\n",
    "        text (list): Contains text in list form\n",
    "        Return:\n",
    "        ret (numpy.array): Indexes of words in text\n",
    "        '''\n",
    "        ret = np.full((max_len+2), padding_value) \n",
    "        for i in range(len(text)):\n",
    "            ret[i] = self.word2idx[text[i]]\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    def indexify_title(self, text, max_len):\n",
    "        '''\n",
    "        Arg:\n",
    "        text (list): Contains title in list form\n",
    "        Return:\n",
    "        ret (numpy.array): Indexes of words in text\n",
    "        '''\n",
    "        ret = np.full((max_len+2), padding_value) \n",
    "        for i in range(len(text)):\n",
    "            ret[i] = self.word2idx[text[i]]\n",
    "            \n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.511183977127075\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ds = CustomDataset(\"data/extracted_comb.npy\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = torch.utils.data.random_split(ds, [round(len(data)*0.2), round(len(data)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x18f70856d08>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,    55,  1262,  ...,     0,     0,     0],\n",
      "        [    1,   325,  1423,  ...,     0,     0,     0],\n",
      "        [    1,    31,  2278,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,  1057,  4651,  ...,     0,     0,     0],\n",
      "        [    1, 10630,    87,  ...,     0,     0,     0],\n",
      "        [    1,    44,  1056,  ...,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "torch.Size([32, 41])\n",
      "tensor([[    1,  2375, 18182,  ...,     0,     0,     0],\n",
      "        [    1,   138,  1705,  ...,     0,     0,     0],\n",
      "        [    1,  1095,   972,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,  1031,    55,  ...,     0,     0,     0],\n",
      "        [    1,  3897,   549,  ...,     0,     0,     0],\n",
      "        [    1,   388,   459,  ...,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "torch.Size([32, 131])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]], device='cuda:0')\n",
      "torch.Size([32, 131])\n"
     ]
    }
   ],
   "source": [
    "for i, hopo in enumerate(trainloader):\n",
    "    if i==1:\n",
    "        break\n",
    "    print(hopo[0])\n",
    "    print(hopo[0].shape)\n",
    "    print(hopo[1])\n",
    "    print(hopo[1].shape)\n",
    "    print(hopo[2])\n",
    "    print(hopo[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding embedding layer for BERT. It needs:\n",
    "\n",
    "* embedding for input\n",
    "* positional embedding\n",
    "* Segment embeddings???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_vocab_size: Number of words in the source vocabulary.\n",
    "          n_blocks: Number of EncoderBlock blocks.\n",
    "          n_features: Number of features to be used for word embedding and further in all layers of the encoder.\n",
    "          n_heads: Number of attention heads inside the EncoderBlock.\n",
    "          n_hidden: Number of hidden units in the Feedforward block of EncoderBlock.\n",
    "          dropout: Dropout level used in EncoderBlock.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = nn.Embedding(src_vocab_size, n_features)\n",
    "        self.pos = tr.PositionalEncoding(n_features, dropout, MAX_LENGTH)\n",
    "        self.blocks = nn.ModuleList([copy.deepcopy(EncoderBlock(n_features, n_heads, n_hidden, dropout)) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (max_seq_length, batch_size): LongTensor with the input sequences.\n",
    "          mask of shape (batch_size, max_seq_length): BoolTensor indicating which elements should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequence.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.emb(x)\n",
    "        embedded = self.pos(embedded)+ embedded\n",
    "        for layer in self.blocks:\n",
    "            embedded = layer(embedded, mask)\n",
    "        return embedded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
