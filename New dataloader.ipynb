{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57834970",
   "metadata": {},
   "source": [
    "# New Dataloader for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25a010",
   "metadata": {},
   "source": [
    "Huggingface BERT needs inputs only as text. Uses BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "395c4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1430426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52c09f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/articles_comb.npy', allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cec87f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142568, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a7da151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'North Korean prison camp survivor changes story'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[40000,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c554ebe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(CNN) Shin ’s horrific descriptions of his time in a North Korean prison camp became a book, made him a key witness before the United Nations and grabbed headlines around the world. He was one of the most North Korean defectors, winning several human rights awards and inspiring a documentary'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[40000,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20478775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142568"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c8dbe99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Among',\n",
       " 'Deaths',\n",
       " 'in',\n",
       " '2016,',\n",
       " 'a',\n",
       " 'Heavy',\n",
       " 'Toll',\n",
       " 'in',\n",
       " 'Pop',\n",
       " 'Music',\n",
       " '-',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d69f1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_title = 0\n",
    "max_text = 0\n",
    "for i in range(len(data)):\n",
    "    title = data[i][0].split()\n",
    "    if max_title< len(title):\n",
    "        max_title = len(title)\n",
    "    \n",
    "    text = data[i][1].split()\n",
    "    if max_text< len(text):\n",
    "        max_text = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ff889aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42b64787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594629da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067cc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052ac4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7291a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, pt_model = 'bert-base-uncased'):\n",
    "        '''\n",
    "        Args:\n",
    "        root_dir (string): Path to npy directory\n",
    "        pt_model (string): Which pre-trained model to use\n",
    "        \n",
    "        Outputs:\n",
    "        title (torch.tensor): indexed input for model\n",
    "        text (torch.tensor): indexed target for model\n",
    "        title_mask (torch.tensor): masking for titles\n",
    "        text_mask (torch.tensor): masking for text\n",
    "        '''\n",
    "        self.data = np.load(root_dir, allow_pickle=True) \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pt_model)\n",
    "        #self.max_len_title = max_title\n",
    "        #self.max_len_text = max_text\n",
    "        #self.word2idx, self.idx2word = self.indexify_vocab(self.data)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #load text and title data and tokenize with ready-made tokenizer\n",
    "        #tokenized_title = self.tokenizer(self.data[idx, 0], return_tensors='pt', padding=True)\n",
    "        #tokenized_text = self.tokenizer(self.data[idx, 1], return_tensors='pt',padding = True)\n",
    "        \n",
    "        # get indexed text & title\n",
    "        #title = tokenized_title['input_ids']\n",
    "        #text = tokenized_text['input_ids']\n",
    "        \n",
    "        # masking for text (+2 in torch.ones comes from adding start and stop tokens)\n",
    "        #title_mask = tokenized_title['attention_mask']\n",
    "        #text_mask = tokenized_text['attention_mask']\n",
    "        \n",
    "        #convert to tensors\n",
    "        #title = title.to(device)\n",
    "        #text = text.to(device)\n",
    "        \n",
    "        title = self.data[idx, 0]\n",
    "        text = self.data[idx, 1]\n",
    "        \n",
    "        return title, text  #, title_mask, text_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeab3e9",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccf6b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.053868055343628\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ds = CustomDataset(\"data/articles_comb.npy\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c71cee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = torch.utils.data.random_split(ds, [round(len(data)*0.2), round(len(data)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b8ea024",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "48fc760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('On The State Dinner Guest List, A ’Letter Writer’ Ignites Imaginations', 'Thirteen killed, 31 injured in California tour bus crash', 'Trump in Mexico: ’Who Pays for the Wall? We Didn’t Discuss it’', 'Artist Accused of Disowning a Painting Testifies - The New York Times', 'One Police Shift: Patrolling an Anxious America - The New York Times', 'WATCH: Apple Stores in Bay Area Targeted by Gangs of Thieves - Breitbart', 'U.S. Open 2015: Jordan Spieth wins second major title', 'The ‘Strength’ of Vladimir Putin')\n",
      "('The White House state dinner in honor of Canada’s prime minister had an exclusive guest list: philanthropists, business leaders, political powerhouses, one public radio host, ambassadors, movie stars. And then, right between The Honorable Patrick Leahy, U. S. Senator (Vermont) and The Honorable Jacob Lew, Secretary of the Treasury, there', 'A tour bus crashed into the back of a on a Southern California highway before dawn on Sunday, killing 13 people and injuring 31, authorities said. The bus was traveling west on Interstate 10 when the crash occurred near Palm Springs, a city about 100 miles (160 km) east of', 'For us to continue writing great stories, we need to display ads. Please select the extension that is blocking ads. Please follow the steps below, Updated on August 31 at 7:07 p. m. On an ordinary day, Donald Trump can’t stop talking about the wall. His promise to build a', 'CHICAGO — The artist Peter Doig took the stand here Monday in an odd federal court case in which the owner of a landscape painting is accusing Mr. Doig of falsely denying that he created the work while a young man in Canada. In a brief opening statement, William F.', 'Policing in America today is a rib dinner paid for by a stranger, and a protester kicking a dent into your patrol car door. It’s warning a young man speeding down a country road to beware of errant deer, and searching through trash cans for a gun on the streets', 'Several Apple stores have been targeted by a gang of thieves who have been caught on camera looting and running with tens of thousands of dollars worth of electronics merchandise. [In an attempt to put a stop to these thefts from happening in the future, the San Francisco police have', '(CNN) Masters champion Jordan Spieth kept his nerve to claim the U. S. Open title Sunday in a dramatic finish at Chambers Bay. Spieth had birdied the final hole to finish on 275, but then watched as fellow American Dustin Johnson from 12 feet on the 18th to finish one', 'Donald Trump’s lamentable habit of batting his eyelashes flirtatiously at frequently shirtless Russian pinup girl Vladimir Putin has produced predictable results, in the form of qualified praise for the Moscow autocrat. You’ll be familiar with the formulation: “Oh, he’s wicked, to be sure, and no friend of the United States,')\n"
     ]
    }
   ],
   "source": [
    "for i, hopo in enumerate(trainloader):\n",
    "    if i==1:\n",
    "        break\n",
    "    print(hopo[0])\n",
    "    print(hopo[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2c091cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sources: Russian Government Intercepted and Will Release Hillary’s Emails - Breitbart'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopo[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5f84081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON DC — Sources are reporting that the Russian government is prepared to release private emails that it obtained from Hillary Clinton’s email server, proving Clinton allowed classified information to end up in the hands of foreign adversaries. [Intelligence sources are bracing for the Russian release of Clinton’s intercepted emails,'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopo[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ccb45df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ed351c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is the best way to escape bad Christmas\\xa0music',\n",
       " 'Father’s Day Fast Facts',\n",
       " 'Poll: Donald Trump Surges to New High, Doubling Closest Competitor - Breitbart',\n",
       " 'Joe Girardi’s magic is gone — and so is his\\xa0patience',\n",
       " 'Sources: Russian Government Intercepted and Will Release Hillary’s Emails - Breitbart',\n",
       " 'Human rights groups vow to challenge burkini ban on Cannes beaches',\n",
       " '17 Colombian Nationals Arrested for Burglarizing Houston Communities',\n",
       " 'WSJ: Donald Trump Overhauls Campaign Team Stephen K. Bannon Named CEO - Breitbart')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "54eca7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_target = tokenizer(hopo[0], return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0031a5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2190,  2126,  2000,  4019,  2919,  4234,\n",
       "          2189,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2269,  1521,  1055,  2154,  3435,  8866,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  8554,  1024,  6221,  8398, 12058,  2015,  2000,  2047,  2152,\n",
       "          1010, 19383,  7541, 12692,  1011,  7987, 20175,  8237,  2102,   102,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  3533, 21025, 25561,  2072,  1521,  1055,  3894,  2003,  2908,\n",
       "          1517,  1998,  2061,  2003,  2010, 11752,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  4216,  1024,  2845,  2231, 16618,  1998,  2097,  2713, 18520,\n",
       "          1521,  1055, 22028,  1011,  7987, 20175,  8237,  2102,   102,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2529,  2916,  2967, 19076,  2000,  4119, 20934, 26891,  2072,\n",
       "          7221,  2006, 14775, 12212,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2459, 13598, 10342,  4727,  2005, 20934, 10623,  8017,  6026,\n",
       "          5395,  4279,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1059,  2015,  3501,  1024,  6221,  8398, 18181,  2015,  3049,\n",
       "          2136,  4459,  1047,  1012,  7221,  8540,  2315,  5766,  1011,  7987,\n",
       "         20175,  8237,  2102,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc055ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
